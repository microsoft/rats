{
    "cells": [
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Intermediate Tutorial\n",
                "In the [beginners tutorial](beginners.md) we have seen how to construct `hello_world`, `diamond`\n",
                "and `standardized_lr` pipelines.\n",
                "In this tutorial we will dive deeper into some of the concepts we touched upon and discuss more\n",
                "complicated use cases."
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### Table of Contents\n",
                "- [Defining Tasks](#defining-tasks)\n",
                "  - [Constructor Parameters](#constructor-parameters)\n",
                "  - [Dynamic Annotations](#dynamic-annotations)\n",
                "- [Combining Pipelines](#combining-pipelines)\n",
                "    - [Parameter Entries](#parameter-entries)\n",
                "    - [Parameter Collections](#parameter-collections)\n",
                "    - [Parameter Types](#parameter-types)\n",
                "    - [Defaults for `UserInput` and `UserOutput`](#defaults-for-userinput-and-useroutput)\n",
                "- [Estimators](#estimators)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Defining Tasks\n",
                "A *task* is a pipeline of a single node.\n",
                "Creating a task requires specifying the pararameters needed for the *processor* to be\n",
                "initialized, i.e., configuration, dynamic input and output annotations, and compute\n",
                "requirements.\n",
                "A *task* has the following construct parameters:\n",
                "* `processor_type` (`type[oneml.processors.IProcess]`): a reference to the processor's class.\n",
                "* `name` (`str`): \\[optional\\] name of the task. If missing, `processor_type.__name__` is used.\n",
                "* `params_getter` (`oneml.processors.IGetParams`): \\[optional\\] a (serializable) *mapping* object\n",
                "    whose keys & values are called to construct `processor_type` before executing it.\n",
                "    Data dependencies will be grabbed automatically from the outputs of other processors, but any\n",
                "    other missing parameters for the constructor need to be specified here.\n",
                "* `input_annotation` (`Mapping[str, type]`): \\[optional\\] dynamic inputs for variable keyword\n",
                "    parameter, e.g., `**kwargs`; required if *processor* specifies\n",
                "    [var keyword](https://docs.python.org/3/library/inspect.html#inspect.Parameter.kind)\n",
                "    parameters.\n",
                "* `return_annotation` (`Mapping[str, type]`): \\[optional\\] dynamic outputs; overrides the\n",
                "    *processors* return annotation. Useful when the number of outputs a processor returns varies\n",
                "    between pipelines, and only known at build time, e.g., a data splitter for cross-validation.\n",
                "* `compute_requirements` (`oneml.processors.PComputeReqs`): \\[optional\\] stores information about\n",
                "    the resources the *processor* needs to run, e.g., CPUs, GPUs, memory, etc."
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Constructor Parameters\n",
                "The following example implements a data loader.\n",
                "This example gets its constructor parameters from an in-memory immutable dictionary, i.e.,\n",
                "`frozendict`.\n",
                "Other mechanims for passing arguments are also supported, e.g., instantiating objects from\n",
                "configuration."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from typing import Any, Mapping, TypedDict\n",
                "\n",
                "from oneml.processors import PipelineBuilder, frozendict\n",
                "\n",
                "LoadDataOut = TypedDict(\"LoadDataOut\", {\"data\": Any})\n",
                "\n",
                "\n",
                "class LoadData:\n",
                "    def __init__(self, storage: Mapping[str, float]):\n",
                "        self._storage = storage\n",
                "\n",
                "    def process(self, key: str) -> LoadDataOut:\n",
                "        return LoadDataOut(data=self._storage[\"key\"])\n",
                "\n",
                "\n",
                "storage = frozendict({\"X_train\": 5, \"X_eval\": 1, \"Y_train\": 0, \"Y_eval\": 1})\n",
                "load_data = PipelineBuilder.task(processor_type=LoadData, name=\"load_data\", params_getter=storage)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "> :bulb: **Info**:\n",
                "Data structure `frozendict` is an immutable and serializable *mapping* object.\n",
                "Immutability, serializability and following the *mapping* interface constitute the requirements for\n",
                "providing configuration into *processors*.\n",
                "Overall, `params_getter` follows the\n",
                "`oneml.processors.IGetParams` [interface](contributor.md#defining-params_getters)."
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Dynamic Annotations\n",
                "Consider the following *processor*:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from typing import Any, Mapping\n",
                "\n",
                "\n",
                "class Identity:\n",
                "    def process(self, **kwargs) -> Mapping[str, Any]:\n",
                "        return kwargs"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "It is a simple processor that returns all parameters it receives.\n",
                "It implements the *process* method and returns a `Mapping` with annotated variables.\n",
                "However, `Identity` does not specify the name of the variables that it expects or returns, and we\n",
                "do not know them a priori.\n",
                "That is the reason why `oneml.processors.Task` accepts `input_annotation` and `return_annotation`\n",
                "parameters.\n",
                "For most *processors* these arguments will be unncessary, but if we provide them, they will\n",
                "override the class's input or return signatures, respectively.\n",
                "For example,"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from oneml.processors import PipelineBuilder\n",
                "\n",
                "io_annotation = {\"foo\": str, \"boo\": bool, \"bee\": int}\n",
                "t = PipelineBuilder.task(Identity, input_annotation=io_annotation, return_annotation=io_annotation)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "> :notebook: **Note**:\n",
                "Passing `return_annotation=None` (default) will not override the *processor's* return signature.\n",
                "Only if the return signature of a processor AND if `return_annotation=None`, will the framework\n",
                "assume that the *processor* return type is actually `None`.\n",
                "Same applies to `input_annotation=None`."
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Combining Pipelines\n",
                "We will further explain how to combine pipelines, handle name conflicts, and expose inputs and\n",
                "outputs."
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### PipelineBuilder.combine\n",
                "The exact parameter signature of `PipelineBuilder.combine` is the following:\n",
                "* `*pipelines` (`Pipeline`): a sequence of `Pipeline`s to combine; must have different names.\n",
                "* `name` (`str`): name of the returned, combined pipeline.\n",
                "* `dependencies` (`Iterable[Sequence[oneml.ux.Dependency]]`): dependencies between the pipelines to\n",
                "    combine, e.g., `stz_eval.inputs.mean << stz_train.outputs.mean`.\n",
                "* `inputs` (`oneml.ux.UserInput | None`): mapping names of inputs and in_collectins of the\n",
                "  pipelines to combine.\n",
                "* `outputs` (`oneml.ux.UserOutput | None`): mapping names of outputs and out_collectins of the\n",
                "  pipelines to combine.\n",
                "Arguments `inputs` and `outputs` are optional and help configure the exposed input and output\n",
                "variables of the combined pipeline.\n",
                "The exact definitions are the following:\n",
                "* `UserInput = Mapping[str, oneml.ux.InEntry | oneml.ux.Inputs]`\n",
                "* `UserOutput = Mapping[str, oneml.ux.OutEntry | oneml.ux.Outputs]`\n",
                "Default behavior for `inputs` and `outputs` set to None is explained in the\n",
                "[section](#defaults-for-userinput-and-useroutput) below.\n",
                "> :notebook: **Note:**\n",
                "Dependencies are not expected to be created manually, but through the `<<` operator syntax."
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### Requirements:\n",
                "* All *inputs* to the pipeline must be specified if `inputs` is not `None`.\n",
                "* Exposing `outputs` is optional.\n",
                "* `UserInput` or `UserOutput` can have at most single dot in the *mapping* keys.\n",
                "An error will be raised otherwise.\n",
                "Consider the following `standardization` example:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from typing import TypedDict\n",
                "\n",
                "from oneml.processors import PipelineBuilder\n",
                "\n",
                "StandardizeTrainOut = TypedDict(\"StandardizeTrainOut\", {\"mean\": float, \"scale\": float, \"Z\": float})\n",
                "StandardizeEvalOut = TypedDict(\"StandardizeEvalOut\", {\"Z\": float})\n",
                "\n",
                "\n",
                "class StandardizeTrain:\n",
                "    def process(X: float) -> StandardizeTrainOut:\n",
                "        ...\n",
                "\n",
                "\n",
                "class StandardizeEval:\n",
                "    def __init__(self, mean: float, scale: float) -> None:\n",
                "        ...\n",
                "\n",
                "    def process(X: float) -> StandardizeEvalOut:\n",
                "        ...\n",
                "\n",
                "\n",
                "stz_train = PipelineBuilder.task(StandardizeTrain)\n",
                "stz_eval = PipelineBuilder.task(StandardizeEval)\n",
                "\n",
                "standardization = PipelineBuilder.combine(\n",
                "    stz_train,\n",
                "    stz_eval,\n",
                "    name=\"standardization\",\n",
                "    dependencies=(\n",
                "        stz_eval.inputs.mean << stz_train.outputs.mean,\n",
                "        stz_eval.inputs.scale << stz_train.outputs.scale,\n",
                "    ),\n",
                "    inputs={\"X.train\": stz_train.inputs.X, \"X.eval\": stz_eval.inputs.X},\n",
                "    outputs={\n",
                "        \"mean\": stz_train.outputs.mean,\n",
                "        \"scale\": stz_train.outputs.scale,\n",
                "        \"Z.train\": stz_train.outputs.Z,\n",
                "        \"Z.eval\": stz_eval.outputs.Z,\n",
                "    },\n",
                ")"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Let's unwrap the example below."
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Parameter Entries\n",
                "We have specified dependencies between the `stz_train` and `stz_eval` using the `<<` operator:\n",
                "```python\n",
                "stz_eval.inputs.mean << stz_train.outputs.mean,\n",
                "stz_eval.inputs.scale << stz_train.outputs.scale,\n",
                "```\n",
                "We access single inputs and outputs of the pipelines via the `inputs` and `outputs` attributes.\n",
                "This happens with `stz_eval.inputs.mean` and `stz_train.outputs.mean`, for example\n",
                "In [begginer's tutorial](beginners.md) we saw another example:\n",
                "```python\n",
                "stz_lr_dependencies = (\n",
                "    logistic_regression.inputs.X_train << standardization.outputs.Z_train,\n",
                "    logistic_regression.inputs.X_eval << standardization.outputs.Z_eval,\n",
                ")\n",
                "```"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "> :notebook: **Note**:\n",
                "In these examples *processors*' inputs and outputs had unique names.\n",
                "If the variable parameters don't have unique names, we can expose them as collections of\n",
                "parameters, as explained in the next section."
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Parameter Collections\n",
                "Pipelines `stz_train` and `stz_eval` both expose `X` and `Z` variable names so we needed to\n",
                "specify how `X` and `Z` are combined together.\n",
                "In the `PipelineBuilder.combine` call we specified:\n",
                "```python\n",
                "inputs={\"X.train\": stz_train.inputs.X, \"X.eval\": stz_eval.inputs.X},\n",
                "outputs={\"Z.train\": stz_train.outputs.Z, \"Z.eval\": stz_eval.outputs.Z}\n",
                "```"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "The combined pipeline will expose variables as collections of parameters:\n",
                "```python\n",
                "standardization.in_collections.X  # Inputs object\n",
                "standardization.out_collections.Z  # Outputs object\n",
                "```"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Both `standardization.in_collections.X` and `standardization.out_collections.Z` gather two inputs\n",
                "and two outputs entries, respectively.\n",
                "The individual entries are accessible via:\n",
                "```python\n",
                "standardization.in_collections.X.train  # InEntry objects\n",
                "standardization.in_collections.X.eval\n",
                "standardization.out_collections.Z.train  # OutEntry objects\n",
                "standardization.out_collections.Z.eval\n",
                "```"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "The usefulness of collections is that we can group parameters together and create dependencies.\n",
                "Here another example with `logistic_regression`:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "LogisticRegressionTrainOut = TypedDict(\n",
                "    \"LogisticRegressionTrainOut\", {\"model\": tuple[float], \"Z\": float}\n",
                ")\n",
                "LogisticRegressionEvalOut = TypedDict(\"LogisticRegressionEvalOut\", {\"Z\": float, \"probs\": float})\n",
                "\n",
                "\n",
                "class LogisticRegressionTrain:\n",
                "    def process(X: float, Y: float) -> LogisticRegressionTrainOut:\n",
                "        ...\n",
                "\n",
                "\n",
                "class LogisticRegressionEval:\n",
                "    def __init__(self, model: tuple[float, ...]) -> None:\n",
                "        ...\n",
                "\n",
                "    def process(X: float, Y: float) -> LogisticRegressionEvalOut:\n",
                "        ...\n",
                "\n",
                "\n",
                "lr_train = PipelineBuilder.task(LogisticRegressionTrain, name=\"lr_train\")\n",
                "lr_eval = PipelineBuilder.task(LogisticRegressionEval, name=\"lr_eval\")\n",
                "\n",
                "logistic_regression = PipelineBuilder.combine(\n",
                "    lr_train,\n",
                "    lr_eval,\n",
                "    name=\"logistic_regression\",\n",
                "    dependencies=(lr_eval.inputs.model << lr_train.outputs.model,),\n",
                "    inputs={\n",
                "        \"X.train\": lr_train.inputs.X,\n",
                "        \"X.eval\": lr_eval.inputs.X,\n",
                "        \"Y.train\": lr_train.inputs.Y,\n",
                "        \"Y.eval\": lr_eval.inputs.Y,\n",
                "    },\n",
                "    outputs={\n",
                "        \"Z.train\": lr_train.outputs.Z,\n",
                "        \"Z.eval\": lr_eval.outputs.Z,\n",
                "        \"model\": lr_train.outputs.model,\n",
                "        \"probs\": lr_eval.outputs.probs,\n",
                "    },\n",
                ")"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "The inputs and outputs automatically formed after combining `lr_train` and `lr_eval` into\n",
                "`logistic_regression` pipeline are:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "logistic_regression.outputs.probs  # OutEntry object\n",
                "\n",
                "logistic_regression.in_collections.X  # Inputs objects\n",
                "logistic_regression.in_collections.Y\n",
                "logistic_regression.out_collections.Z  # Outputs object\n",
                "\n",
                "logistic_regression.in_collections.X.train  # InEntry objects\n",
                "logistic_regression.in_collections.X.eval\n",
                "logistic_regression.in_collections.Y.train\n",
                "logistic_regression.in_collections.Y.eval\n",
                "logistic_regression.out_collections.Z.train  # OutEntry objects\n",
                "logistic_regression.out_collections.Z.eval"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Finally, we can combine `standardization` and `logistic_regression` together:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "stz_lr = PipelineBuilder.combine(\n",
                "    standardization,\n",
                "    logistic_regression,\n",
                "    name=\"stz_lr\",\n",
                "    # two dependencies returned from input-output collection assignment\n",
                "    dependencies=(logistic_regression.in_collections.X << standardization.out_collections.Z,),\n",
                ")"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Notice the simple assignment of the `stz_lr` pipeline's input and output collections.\n",
                "This mechanism generalizes to any number of parameters that share the same entry names, which is\n",
                "useful for operating on pipelines with varying number of entries."
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Parameter Types\n",
                "Accessing `inputs` and `outputs` attributes of a pipeline returns a mapping of entry parameters;\n",
                "accessing `in_collections` and `out_collections` returns a mapping of collections of parameters,\n",
                "whose values are mappings of entry parameters.\n",
                "These are the types from the nested mappings, accessible via dot notation for some examples we have\n",
                "seen above:\n",
                "|     `Pipeline`    |     `Inputs`       |    `InEntry`    |\n",
                "|:-----------------:|:------------------:|:---------------:|\n",
                "|    `stz_train`    |     `.inputs`      |    `.X`         |\n",
                "|    `stz_eval`     |     `.inputs`      |    `.X`         |\n",
                "|    `stz_eval`     |     `.inputs`      |    `.mean`      |\n",
                "|    `stz_eval`     |     `.inputs`      |    `.scale`     |\n",
                "\n",
                "|     `Pipeline`    |     `Outputs`      |    `OutEntry`   |\n",
                "|:-----------------:|:------------------:|:---------------:|\n",
                "|     `stz_train`   |     `.outputs`     |    `.Z`         |\n",
                "|     `stz_train`   |     `.outputs`     |    `.mean`      |\n",
                "|     `stz_train`   |     `.outputs`     |    `.scale`     |\n",
                "|     `stz_eval`    |     `.outputs`     |    `.Z`    |\n",
                "\n",
                "\n",
                "The pipeline exposing collections of entries:\n",
                "\n",
                "|     `Pipeline`    |  `InCollections`   |  `InCollection` | `InEntry`  |\n",
                "|:-----------------:|:------------------:|:---------------:|:----------:|\n",
                "| `standardization` | `.in_collections`  |      `.X`       |  `.train`  |\n",
                "| `standardization` | `.in_collections`  |      `.X`       |  `.eval`   |\n",
                "\n",
                "|     `Pipeline`    |  `OutCollections`  | `OutCollection` | `OutEntry` |\n",
                "|:-----------------:|:------------------:|:---------------:|:----------:|\n",
                "| `standardization` | `.out_collections` |      `.Z`       |  `.train`  |\n",
                "| `standardization` | `.out_collections` |      `.Z`       |  `.eval`   |"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Defaults for `UserInput` and `UserOutput`\n",
                "Leaving `inputs` and `outputs` of `PipelineBuilder.combine` unspecified or set to `None` will\n",
                "default their specificiation::\n",
                "* All `inputs` or `outputs` from pipelines to combine will be merged, after subtracting any input\n",
                "or output specified in the `dependencies` argument.\n",
                "For example,"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Leaving `inputs=None` is equivalent to"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "inputs = {\"X\": standardization.in_collections.X, \"Y\": logistic_regression.in_collections.Y}"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Entry `logistic_regression.inputs.X` is specified as a dependency, and is therefore not included\n",
                "in the inputs of the combined pipeline.\n",
                "Leaving `outputs=None` is equivalent to"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "outputs = {\n",
                "    \"mean\": standardization.outputs.mean,\n",
                "    \"scale\": standardization.outputs.scale,\n",
                "    \"model\": logistic_regression.outputs.model,\n",
                "    \"probs\": logistic_regression.outputs.probs,\n",
                "}"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Only `standardization.outputs.Z` is specified in the dependencies list and excluded."
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Estimators\n",
                "[Combining pipelines](#combining-pipelines) for creating train and eval tasks so far:\n",
                "1. Instantiate tasks, e.g., `stz_train`, `stz_eval`, `lr_train`, `lr_eval`.\n",
                "2. Combine tasks, e.g., `standardization`, `logistic_regression`.\n",
                "3. Specify dependencies, e.g., `stz_eval.inputs.mean << stz_train.outputs.mean`,\n",
                "`lr_eval.inputs.model << lr_train.outputs.model`.\n",
                "4. Specify inputs and outputs, e.g.,\n",
                "`inputs={\"X.train\": stz_train.inputs.X, \"X.eval\": stz_eval.inputs.X}`.\n",
                "5. Combine pipelines passing dependencies, inputs and outputs.\n",
                "This pattern can be simplified with estimators:\n",
                "1. Instantiate tasks, e.g., `stz_train`, `stz_eval`, `lr_train`, `lr_eval`.\n",
                "2. Instantiate estimators, e.g., `standardization`, `logistic_regression`.\n",
                "4. Combine estimators, e.g., `stz_lr` below."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from oneml.processors.ml import Estimator\n",
                "\n",
                "# Instantiate tasks\n",
                "stz_train = PipelineBuilder.task(StandardizeTrain)\n",
                "stz_eval = PipelineBuilder.task(StandardizeEval)\n",
                "lr_train = PipelineBuilder.task(LogisticRegressionTrain)\n",
                "lr_eval = PipelineBuilder.task(LogisticRegressionEval)\n",
                "\n",
                "# Instantiate estimators\n",
                "standardization = Estimator(\n",
                "    name=\"standardization\",\n",
                "    train_pl=stz_train,\n",
                "    eval_pl=stz_eval,\n",
                "    dependencies=(\n",
                "        stz_eval.inputs.mean << stz_train.outputs.mean,\n",
                "        stz_eval.inputs.scale << stz_train.outputs.scale,\n",
                "    ),\n",
                ")\n",
                "logistic_regression = Estimator(\n",
                "    name=\"logistic_regression\",\n",
                "    train_pl=lr_train,\n",
                "    eval_pl=lr_eval,\n",
                "    dependencies=(lr_eval.inputs.model << lr_train.outputs.model,),\n",
                ")\n",
                "\n",
                "# Combine estimators\n",
                "stz_lr = PipelineBuilder.combine(\n",
                "    standardization,\n",
                "    logistic_regression,\n",
                "    name=\"stz_lr\",\n",
                "    dependencies=(logistic_regression.in_collections.X << standardization.out_collections.Z,),\n",
                ")"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "The main advantage for using estimators is that collections are built so that they are compatible\n",
                "to operate together.\n",
                "If the previous *processors* had more than one evaluation task, the underlying dependency\n",
                "assignments do not change if they rely on collection assignments.\n",
                "> :notebook: **Note:**\n",
                "Combinging pipelines using `Estimator` exposes all outputs from both pipelines.\n",
                "That is why `standardization` exposes `mean` and `scale`, and why `logistic_regression` exposes\n",
                "`model`, even though they are specified as dependencies.\n",
                "This is not the default behavior of `PipelineBuilder.combine`, which does not expose outputs that\n",
                "have been specified as dependencies."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "oneml-app-HvMeh-nl-py3.10",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.6"
        },
        "orig_nbformat": 4,
        "vscode": {
            "interpreter": {
                "hash": "118274ffcdc0169b3c4a867cb488386328c6727e3b0844e9bd2c3e369f2ace34"
            }
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
